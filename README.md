# Integrated-Data-Processing-Pipeline-Project
Just built the full data pipeline! First, my Python scraper auto-collected images from the web. Then, I meticulously annotated them in Labelme for AI training. Finally, I visualized the dataset statistics with Matplotlib charts. This hands-on experiment gave me a complete, practical view of the data science workflow from raw data to insight.


This integrated project offers a comprehensive, hands-on journey through the essential data science pipeline, beginning with Task 1, where students master web scraping by automatically extracting images from the web to understand HTTP protocols and automated data collection. The workflow transitions to Task 2, focusing on image scene segmentation and annotation using Labelme, where students gain the critical skill of creating high-quality labeled datasets necessary for training precise computer vision models. Finally, Task 3 emphasizes data visualization through Matplotlib, teaching students to transform raw information into insightful bar charts and bubble plots to facilitate better data interpretation. By navigating these three distinct phases—acquisition, preparation, and presentation—participants bridge the gap between theoretical concepts and practical application in modern AI workflows. This progression ensures a holistic understanding of how raw digital assets are transformed into structured, actionable intelligence through systematic processing. Ultimately, the project demonstrates the fundamental synergy between automated crawling, manual expert labeling, and graphical analysis in solving complex technical challenges.

# Task 1: Network Crawling Images
Technology sued: 
Language : Python 3.12 
Library: requests library version 2.31.0

